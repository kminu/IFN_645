{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snsa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from dm_tool import data_prep \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=20):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    print(\"\\n\\n*********** Feature Importances ************\\n\")   \n",
    "    for i in indices:\n",
    "        print(f\"{feature_names[i]:<35}:{importances[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_tree(dm_model, feature_names, save_name):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(dm_model, out_file=dotfile, feature_names=feature_names, filled=True, rounded=True)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png(save_name) # saved in the following file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_count(model):\n",
    "    n_nodes = model.tree_.node_count\n",
    "    children_left = model.tree_.children_left\n",
    "    children_right = model.tree_.children_right\n",
    "    feature = model.tree_.feature\n",
    "    threshold = model.tree_.threshold\n",
    "    \n",
    "    # The tree structure can be traversed to compute various properties such\n",
    "    # as the depth of each node and whether or not it is a leaf.\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "        # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(\"The binary tree structure has %s nodes\" % n_nodes)\n",
    "    \n",
    "   # Count decision nodes\n",
    "    n_count = 0\n",
    "    for i in range(n_nodes):\n",
    "        if is_leaves[i]:\n",
    "            n_count = n_count + 1\n",
    "    print(\"The tree model has %s decision nodes\" % n_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_node_number = 0\n",
    "leaf_node_number = 0\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        leaf_node_number = leaf_node_number + 1\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        decision_node_number = decision_node_number + 1\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steven\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3185: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "# preprocessing step\n",
    "df = data_prep()\n",
    "\n",
    "# target/input split\n",
    "y = df['IsBadBuy']\n",
    "X = df.drop(['IsBadBuy'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7511, 125)\n",
      "(3219, 125)\n",
      "(7511,)\n",
      "(3219,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steven\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# setting random state\n",
    "# .as_matrix removed replaced with .values\n",
    "# train test 70 / 30 percent\n",
    "rs = 10\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model construction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple decision tree training with default setting\n",
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# decision tree evaluation\n",
    "print(\"\\n\\n*********** Evaluation of the decision tree ************\\n\") \n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\\n*********** Confusion Matrix ************\\n\") \n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "analyse_feature_importance(model, X.columns, 20)\n",
    "\n",
    "#visualising the model\n",
    "visualize_decision_tree(model, feature_names, \"default.png\")\n",
    "\n",
    "#Size of decision tree model\n",
    "print(\"\\n\\n*********** Size of decision tree ************\\n\") \n",
    "node_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Decition Tree model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*********** Evaluation of the decision tree ************\n",
      "\n",
      "Train accuracy: 0.75902010384769\n",
      "Test accuracy: 0.7567567567567568\n",
      "\n",
      "\n",
      "*********** Confusion Matrix ************\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77      1609\n",
      "           1       0.80      0.68      0.74      1610\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      3219\n",
      "   macro avg       0.76      0.76      0.76      3219\n",
      "weighted avg       0.76      0.76      0.76      3219\n",
      "\n",
      "\n",
      "\n",
      "*********** Feature Importances ************\n",
      "\n",
      "Auction_ADESA                      :0.571435214578175\n",
      "MMRCurrentAuctionAveragePrice      :0.1454371788211704\n",
      "VNST_OK                            :0.09192443745447534\n",
      "VNST_CO                            :0.051861359005029174\n",
      "VNST_AZ                            :0.03242171935598875\n",
      "VNST_PA                            :0.029128640102322405\n",
      "Auction_OTHER                      :0.02742613108261397\n",
      "VehBCost                           :0.025783852802885004\n",
      "VNST_CA                            :0.015410298161252467\n",
      "VNST_TX                            :0.004722860655972494\n",
      "MMRCurrentRetailRatio              :0.004448307980114939\n",
      "Make_LINCOLN                       :0.0\n",
      "Make_LEXUS                         :0.0\n",
      "Make_JEEP                          :0.0\n",
      "Make_KIA                           :0.0\n",
      "Make_MERCURY                       :0.0\n",
      "Make_ISUZU                         :0.0\n",
      "Make_INFINITI                      :0.0\n",
      "Make_HYUNDAI                       :0.0\n",
      "Make_HONDA                         :0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8a6de33e6131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#visualising the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mvisualize_decision_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"second_tree.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Size of decision tree model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "#retrain with a small max_depth limit 5\n",
    "model_2 = DecisionTreeClassifier(max_depth=5, random_state=rs, )\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "# decision tree evaluation\n",
    "print(\"\\n\\n*********** Evaluation of the decision tree ************\\n\") \n",
    "print(\"Train accuracy:\", model_2.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model_2.score(X_test, y_test))\n",
    "\n",
    "#confusoin matrix\n",
    "print(\"\\n\\n*********** Confusion Matrix ************\\n\") \n",
    "y_pred = model_2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "analyse_feature_importance(model_2, X.columns, 20)\n",
    "\n",
    "#visualising the model\n",
    "visualize_decision_tree(model_2, feature_names, \"second_tree.png\")\n",
    "\n",
    "#Size of decision tree model\n",
    "print(\"\\n\\n*********** Size of decision tree ************\\n\") \n",
    "node_count(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the performance of model with different complexity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = []\n",
    "train_score = []\n",
    "\n",
    "# check the model performance for max depth from 2-20\n",
    "for max_depth in range(2, 21):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=rs)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    test_score.append(model.score(X_test, y_test))\n",
    "    train_score.append(model.score(X_train, y_train))\n",
    "    \n",
    "# plot max depth hyperparameter values vs training and test accuracy score\n",
    "plt.plot(range(2, 21), train_score, 'b', range(2,21), test_score, 'r')\n",
    "plt.xlabel('max_depth\\nBlue = training acc. Red = test acc.')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(3, 9),\n",
    "          'min_samples_leaf': range(20, 80, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV #2\n",
    "params = {'criterion': ['gini'],\n",
    "          'max_depth': range(4, 9),\n",
    "          'min_samples_leaf': range(20, 35)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*********** Evaluation of the decision tree ************\n",
      "\n",
      "Train accuracy: 0.7638130741579018\n",
      "Test accuracy: 0.7601739670705188\n",
      "\n",
      "\n",
      "*********** Confusion Matrix ************\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.78      1609\n",
      "           1       0.81      0.68      0.74      1610\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      3219\n",
      "   macro avg       0.77      0.76      0.76      3219\n",
      "weighted avg       0.77      0.76      0.76      3219\n",
      "\n",
      "\n",
      "\n",
      "*********** Feature Importances ************\n",
      "\n",
      "Auction_ADESA                      :0.5422671171500951\n",
      "MMRCurrentAuctionAveragePrice      :0.13801354497205165\n",
      "VNST_OK                            :0.08723228534468135\n",
      "VNST_CO                            :0.04921416972858956\n",
      "VNST_AZ                            :0.030766798824604577\n",
      "VNST_PA                            :0.027641810115690942\n",
      "Auction_OTHER                      :0.026026203246378853\n",
      "VNST_MO                            :0.025090514534735073\n",
      "VehBCost                           :0.02446775272462682\n",
      "VNST_CA                            :0.014623701419056629\n",
      "VNST_NC                            :0.01254757997967377\n",
      "VNST_FL                            :0.006523228827706428\n",
      "MMRCurrentRetailRatio              :0.0062168351504326745\n",
      "VNST_TX                            :0.004481788953987272\n",
      "MMRAcquisitionAuctionAveragePrice  :0.003104321403774169\n",
      "MMRCurrentRetailAveragePrice       :0.0017823476239150367\n",
      "Make_MERCURY                       :0.0\n",
      "Make_MAZDA                         :0.0\n",
      "Make_LINCOLN                       :0.0\n",
      "VNST_WV                            :0.0\n",
      "\n",
      "\n",
      "*********** Size of decision tree ************\n",
      "\n",
      "The binary tree structure has 35 nodes\n",
      "The tree model has 18 leaf nodes\n"
     ]
    }
   ],
   "source": [
    "# Assigining the best parameter to optimal model\n",
    "\n",
    "Optimal_model = DecisionTreeClassifier(criterion = 'gini', splitter = 'best', max_depth = 6,  min_samples_leaf = 26)\n",
    "\n",
    "\n",
    "Optimal_model.fit(X_train, y_train)\n",
    "\n",
    "# decision tree evaluation\n",
    "print(\"\\n\\n*********** Evaluation of the decision tree ************\\n\") \n",
    "print(\"Train accuracy:\", Optimal_model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", Optimal_model.score(X_test, y_test))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\\n*********** Confusion Matrix ************\\n\") \n",
    "y_pred = Optimal_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "analyse_feature_importance(Optimal_model, X.columns, 20)\n",
    "\n",
    "#visualising the model\n",
    "visualize_decision_tree(Optimal_model, X.columns, \"Optimal_model.png\")\n",
    "\n",
    "#Size of decision tree model\n",
    "print(\"\\n\\n*********** Size of decision tree ************\\n\") \n",
    "node_count(Optimal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of decison tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 35 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 13] <= 0.5 else to node 34.\n",
      "\tnode=1 test node: go to node 2 if X[:, 6] <= 4738.0 else to node 25.\n",
      "\t\tnode=2 test node: go to node 3 if X[:, 97] <= 0.5 else to node 16.\n",
      "\t\t\tnode=3 test node: go to node 4 if X[:, 11] <= 4240.0 else to node 11.\n",
      "\t\t\t\tnode=4 test node: go to node 5 if X[:, 120] <= 0.5 else to node 8.\n",
      "\t\t\t\t\tnode=5 test node: go to node 6 if X[:, 98] <= 0.5 else to node 7.\n",
      "\t\t\t\t\t\tnode=6 leaf node.\n",
      "\t\t\t\t\t\tnode=7 leaf node.\n",
      "\t\t\t\t\tnode=8 test node: go to node 9 if X[:, 2] <= 3358.0 else to node 10.\n",
      "\t\t\t\t\t\tnode=9 leaf node.\n",
      "\t\t\t\t\t\tnode=10 leaf node.\n",
      "\t\t\t\tnode=11 test node: go to node 12 if X[:, 96] <= 0.5 else to node 15.\n",
      "\t\t\t\t\tnode=12 test node: go to node 13 if X[:, 107] <= 0.5 else to node 14.\n",
      "\t\t\t\t\t\tnode=13 leaf node.\n",
      "\t\t\t\t\t\tnode=14 leaf node.\n",
      "\t\t\t\t\tnode=15 leaf node.\n",
      "\t\t\tnode=16 test node: go to node 17 if X[:, 15] <= 0.5 else to node 24.\n",
      "\t\t\t\tnode=17 test node: go to node 18 if X[:, 10] <= 0.8278218507766724 else to node 21.\n",
      "\t\t\t\t\tnode=18 test node: go to node 19 if X[:, 8] <= 5535.5 else to node 20.\n",
      "\t\t\t\t\t\tnode=19 leaf node.\n",
      "\t\t\t\t\t\tnode=20 leaf node.\n",
      "\t\t\t\t\tnode=21 test node: go to node 22 if X[:, 10] <= 0.8855109214782715 else to node 23.\n",
      "\t\t\t\t\t\tnode=22 leaf node.\n",
      "\t\t\t\t\t\tnode=23 leaf node.\n",
      "\t\t\t\tnode=24 leaf node.\n",
      "\t\tnode=25 test node: go to node 26 if X[:, 115] <= 0.5 else to node 33.\n",
      "\t\t\tnode=26 test node: go to node 27 if X[:, 95] <= 0.5 else to node 32.\n",
      "\t\t\t\tnode=27 test node: go to node 28 if X[:, 117] <= 0.5 else to node 31.\n",
      "\t\t\t\t\tnode=28 test node: go to node 29 if X[:, 105] <= 0.5 else to node 30.\n",
      "\t\t\t\t\t\tnode=29 leaf node.\n",
      "\t\t\t\t\t\tnode=30 leaf node.\n",
      "\t\t\t\t\tnode=31 leaf node.\n",
      "\t\t\t\tnode=32 leaf node.\n",
      "\t\t\tnode=33 leaf node.\n",
      "\tnode=34 leaf node.\n",
      "\n",
      "Rules used to predict sample 0: \n",
      "decision id node 0 : (X_test[0, 13] (= 0.0) <= 0.5)\n",
      "decision id node 1 : (X_test[0, 6] (= 8260.0) > 4738.0)\n",
      "decision id node 25 : (X_test[0, 115] (= 0.0) <= 0.5)\n",
      "decision id node 26 : (X_test[0, 95] (= 0.0) <= 0.5)\n",
      "decision id node 27 : (X_test[0, 117] (= 0.0) <= 0.5)\n",
      "decision id node 28 : (X_test[0, 105] (= 0.0) <= 0.5)\n",
      "\n",
      "The following samples [0, 1] share the node [ 0  1 25 26 27 28 29] in the tree\n",
      "It is 20.0 % of all nodes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*********** Size of decision tree ************\n",
      "\n",
      "The binary tree structure has 35 nodes and has the following tree structure:\n",
      "The tree has 17 decision nodes.\n",
      "The tree has 18 leaf nodes.\n"
     ]
    }
   ],
   "source": [
    "# The decision estimator has an attribute called tree_  which stores the entire\n",
    "# tree structure and allows access to low level attributes. The binary tree\n",
    "# tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "# array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "# Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "# case the values of nodes of the other type are arbitrary!\n",
    "#\n",
    "# Among those arrays, we have:\n",
    "#   - left_child, id of the left child of the node\n",
    "#   - right_child, id of the right child of the node\n",
    "#   - feature, feature used for splitting the node\n",
    "#   - threshold, threshold value at the node\n",
    "#\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = Optimal_model.tree_.node_count\n",
    "children_left = Optimal_model.tree_.children_left\n",
    "children_right = Optimal_model.tree_.children_right\n",
    "feature = Optimal_model.tree_.feature\n",
    "threshold = Optimal_model.tree_.threshold\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "\n",
    "# Calculate decision node\n",
    "decision_node_number = 0\n",
    "leaf_node_number = 0\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        leaf_node_number = leaf_node_number + 1\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        decision_node_number = decision_node_number + 1\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()\n",
    "\n",
    "    \n",
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "\n",
    "node_indicator = Optimal_model.decision_path(X_test)\n",
    "\n",
    "# Similarly, we can also have the leaves ids reached by each sample.\n",
    "\n",
    "leave_id = Optimal_model.apply(X_test)\n",
    "\n",
    "# Now, it's possible to get the tests that were used to predict a sample or\n",
    "# a group of samples. First, let's make it for the sample.\n",
    "\n",
    "sample_id = 0\n",
    "node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                    node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    if leave_id[sample_id] == node_id:\n",
    "        continue\n",
    "\n",
    "    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "          % (node_id,\n",
    "             sample_id,\n",
    "             feature[node_id],\n",
    "             X_test[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))\n",
    "\n",
    "# For a group of samples, we have the following common node.\n",
    "sample_ids = [0, 1]\n",
    "common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n",
    "                len(sample_ids))\n",
    "\n",
    "common_node_id = np.arange(n_nodes)[common_nodes]\n",
    "\n",
    "print(\"\\nThe following samples %s share the node %s in the tree\"\n",
    "      % (sample_ids, common_node_id))\n",
    "print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"\\n\\n*********** Size of decision tree ************\\n\") \n",
    "print(\"The binary tree structure has %s nodes and has \" \"the following tree structure:\" % n_nodes)\n",
    "print(\"The tree has %s decision nodes.\" % decision_node_number)\n",
    "print(\"The tree has %s leaf nodes.\" % leaf_node_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 35 nodes\n",
      "The tree model has 18 decision nodes\n"
     ]
    }
   ],
   "source": [
    "node_count(Optimal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
